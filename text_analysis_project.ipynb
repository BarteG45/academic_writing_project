{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Academic Writing Project\n",
    "### Text scraping and linguistic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing request module\n",
    "At first, it is important to import the requests module, which allows us to send HTTP requests with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a page data\n",
    "requests.get() method sends a request to the chosen url. We can assign the content to a variable and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(\"https://www.football365.com/news/man-city-ffp-relegation-not-sensationalised-premier-league-desire/\")\n",
    "print(page)\n",
    "print(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup package\n",
    "Here, it is worth mentioning the Wikipedia, which describes the application of the package: \n",
    "\"Beautiful Soup is a Python package for parsing HTML and XML documents, including those with malformed markup. It creates a parse tree for documents that can be used to extract data from HTML, which is useful for web scraping.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding paragraphs in a chosen text\n",
    "To distinguish paragraphs in the text, we need to use \"find_all\" method as below. We will assign this to the \"paras\" variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = soup.find_all('p')\n",
    "print (paras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating the text itself\n",
    "The next step will be very important, as we would like to pull only the text from a chosen website. To do it, we use the for loop along with getText and strip method. The latter removes starting and ending whitespaces from a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_text = []\n",
    "for el in paras:\n",
    "  if len(el.getText().strip()) > 0:\n",
    "    only_text.append(el.getText())\n",
    "print(only_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lingustics tools and analysis\n",
    "Having the text, we can implement several lingustics tools to analyse it in lingustics terms. Our first step in this matter needs to be a nltk (Natural Language Toolkit) package import. Later, we need to download nltk data, using the download method and chosing the data that we are interested in. Below we download the popular ones (nltk.download(\"popular\")). We can, however, download all of them as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in only_text:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence tokenization\n",
    "To tokenize our sentences from a scraped text, we need to use the \"word_tokenize\" method, as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in only_text:\n",
    "  print(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words tagging in a sentence\n",
    "Another useful tool is \"pos_tag\" method that helps us getting parts of speech tags for our sentence. Here, I implement it with a help of a for loop and present it using tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuples = []\n",
    "for sentence in only_text:\n",
    "  tokens = (nltk.word_tokenize(sentence))\n",
    "  pos_tagged = nltk.pos_tag(tokens)\n",
    "  for item in pos_tagged:\n",
    "    tuples.append(item)\n",
    "print(tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of speech frequency (dictionary)\n",
    "WHich of the parts of speech is the most frequent one? We can check it and present it creating a dictionary and adding values using a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_dict = {}\n",
    "\n",
    "for el in tuples:\n",
    "    tag = el[1]\n",
    "    if tag not in counter_dict:\n",
    "      counter_dict[tag] = 1\n",
    "    else:\n",
    "      counter_dict[tag] += 1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating keys and values of the dictionary\n",
    "We can also check and display the keys and values  (separately) of formely created dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = []\n",
    "values = []\n",
    "\n",
    "for el in counter_dict:\n",
    "  keys.append(el)\n",
    "  values.append(counter_dict[el])\n",
    "\n",
    "print(keys)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing \"Stop words\"\n",
    "We can also remove so called \"Stop words\". What are they? According to Kavita Ganesan, \"a set of commonly used words in any language. For example, in English, “the”, “is” and “and”, would easily qualify as stop words. In NLP and text mining applications, stop words are used to eliminate unimportant words, allowing applications to focus on the important words instead.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "to_be_removed = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_para=word_tokenize(str(only_text))\n",
    "print(tokenized_para)\n",
    "modified_token_list=[word for word in tokenized_para if not word in to_be_removed]\n",
    "print(modified_token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data presented by a chart\n",
    "Our data can be accesibly presented using different types of charts, for example. To do so, we need to import matplotlib.pyplot collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presenting data with charts\n",
    "After getting the collection, we can choose bar chart or pie chart, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(keys, values, color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pie(values,labels = keys[0:8])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
